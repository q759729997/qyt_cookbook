==================
NLP数据集
==================

文本数据增强
######################

- 相较于图像数据增强，文本数据增强，现在还是有很多问题的。往更严格的角度看，文本数据增强更像是同义句生成，但又不完全是，它是一个更大范围的概念。很多时候，需要文本数据增强，一个是常常遇到的数据不足，另一个就是数据不均衡。大方向上看，文本的数据增强有几种方式，EDA、Back Translation、生成对抗网络、语境增强。 https://mp.weixin.qq.com/s/aIydEKcDYWNaczMUi07tIQ
- NLP中数据增强的综述，快速的生成大量的训练数据: https://mp.weixin.qq.com/s/an1ewfO6pWyR6QmQyFv3Rw

EDA
***************************

- ICLR 2019 workshop 论文《EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks》介绍了几种NLP数据增强技术，并推出了EDA github代码。EDA github repo提出了四种简单的操作来进行数据增强，以防止过拟合，并提高模型的泛化能力。其实就是EDA的四种形式。

    - 同义词替换（SR: Synonyms Replace）:不考虑stopwords，在句子中随机抽取n个词，然后从同义词词典中随机抽取同义词，并进行替换。
    - 随机插入(RI: Randomly Insert):不考虑stopwords，随机抽取一个词，然后在该词的同义词集合中随机选择一个，插入原句子中的随机位置。该过程可以重复n次。
    - 随机交换(RS: Randomly Swap):句子中，随机选择两个词，位置交换。该过程可以重复n次。
    - 随机删除(RD: Randomly Delete):句子中的每个词，以概率p随机删除。

词汇替换
========================

- 基于词典的替换：在这种技术中，我们从句子中随机取出一个单词，并使用同义词词典将其替换为同义词。例如，我们可以使用WordNet的英语词汇数据库来查找同义词，然后执行替换。它是一个手动管理的数据库，其中包含单词之间的关系。
- 基于词向量的替换：在这种方法中，我们采用预先训练好的单词嵌入，如Word2Vec、GloVe、FastText、Sent2Vec，并使用嵌入空间中最近的相邻单词替换句子中的某些单词。Jiao et al.在他们的论文“TinyBert”中使用了这种技术，以提高他们的语言模型在下游任务上的泛化能力。Wang et al.使用它来增加学习主题模型所需的tweet。
- Masked Language Model：像BERT、ROBERTA和ALBERT这样的Transformer模型已经接受了大量的文本训练，使用一种称为“Masked Language Modeling”的预训练，即模型必须根据上下文来预测遮盖的词汇。这可以用来扩充一些文本。例如，我们可以使用一个预训练的BERT模型并屏蔽文本的某些部分。然后，我们使用BERT模型来预测遮蔽掉的token。
- 基于TF-IDF的词替换：这种增强方法是由Xie et al.在无监督数据增强论文中提出的。其基本思想是，TF-IDF分数较低的单词不能提供信息，因此可以在不影响句子的ground-truth的情况下替换它们。

文本表面转换
========================

- 使用正则表达式的简单的模式匹配的转换

随机噪声注入
========================

- 拼写错误注入：在这种方法中，我们在句子中的一些随机单词上添加拼写错误。这些拼写错误可以通过编程方式添加，也可以使用常见拼写错误的映射。
- QWERTY键盘错误注入：该方法试图模拟在QWERTY布局键盘上输入时发生的常见错误，这些错误是由于按键之间的距离非常近造成的。错误是根据键盘距离注入的。
- 句子打乱：这是一种朴素的技术，我们将训练文本中的句子打乱，以创建一个增强版本。
- Unigram噪声：该方法已被Xie et al.和UDA论文所采用。其思想是用从单字符频率分布中采样的单词进行替换。这个频率基本上就是每个单词在训练语料库中出现的次数。
- Blank Noising：这个方法是由Xie et al.在他们的论文中提出的。其思想是用占位符标记替换一些随机单词。本文使用“_”作为占位符标记。在论文中，他们将其作为一种避免特定上下文过拟合的方法，以及语言模型的平滑机制。该技术有助于提高perplexity和BLEU评分。

回译（Back Translation）
***************************

- 回译（Back Translation）是机器翻译中非常常用的数据增强的方式，其主要的思想就是通过翻译工具将一个句子翻译为另一种语言，再把这翻译的另一种语言再翻译为原来的语言，最后得到一个意思相近但表达方式不同的句子。这种方式也是目前相对靠谱的方式，这种方式不仅有同义词替换，词语增删，还具有对句子结构语序调整的效果，并还能保持与原句子意思相近，是目前一种非常有效的文本数据增强方式。

生成对抗网络
***************************

- 随着深度学习的发展，这几年生成对抗网络模型（GAN）以及它的各种变形，通过生成器和判别器的相互博弈，以达到一个纳什平衡，不断迭代增强训练数据达到以假乱真的效果，最后用生成器大量生成同分布的数据，以达到数据增强的效果。但是GAN模型比较难训练，所以需要对GAN模型训练的足够好，才能更加有效的生成高质量的数据。

情感分类
######################

IMDb
***************************

- 斯坦福的IMDb数据集（Stanford's Large Movie Review Dataset）。这个数据集分为训练和测试用的两个数据集，分别包含25,000条从IMDb下载的关于电影的评论。在每个数据集中，标签为“正面”和“负面”的评论数量相等。
- 下载链接(压缩包越80MB)： http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz

机器翻译
######################

- WMT. http://www.statmt.org/wmt14/translation-task.html
- Tatoeba Project. http://www.manythings.org/anki/