==================
语言模型
==================

- 语言模型（language model）是自然语言处理的重要技术。
- 自然语言处理中最常见的数据是文本数据。我们可以把一段自然语言文本看作一段离散的时间序列。假设一段长度为 :math:`T` 的文本中的词依次为 :math:`w_1, w_2, \ldots, w_T` ，那么在离散的时间序列中， :math:`w_t` （ :math:`1 \leq t \leq T` ）可看作在时间步（time step） :math:`t` 的输出或标签。给定一个长度为 :math:`T` 的词的序列 :math:`w_1, w_2, \ldots, w_T` ，语言模型将计算该序列的概率：

.. math::
	
	P(w_1, w_2, \ldots, w_T)

- 语言模型可用于提升语音识别和机器翻译的性能。例如，在语音识别中，给定一段“厨房里食油用完了”的语音，有可能会输出“厨房里食油用完了”和“厨房里石油用完了”这两个读音完全一样的文本序列。如果语言模型判断出前者的概率大于后者的概率，我们就可以根据相同读音的语音输出“厨房里食油用完了”的文本序列。在机器翻译中，如果对英文“you go first”逐词翻译成中文的话，可能得到“你走先”“你先走”等排列方式的文本序列。如果语言模型判断出“你先走”的概率大于其他排列方式的文本序列的概率，我们就可以把“you go first”翻译成“你先走”。

语言模型的计算
######################

- 假设序列 :math:`w_1, w_2, \ldots, w_T` 中的每个词是依次生成的，我们有

.. math::

	P(w_1, w_2, \ldots, w_T) = \prod_{t=1}^T P(w_t \mid w_1, \ldots, w_{t-1})

- 例如，一段含有4个词的文本序列的概率

.. math::

	P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3)

- 为了计算语言模型，我们需要计算词的概率，以及一个词在给定前几个词的情况下的条件概率，即语言模型参数。设训练数据集为一个大型文本语料库，如维基百科的所有条目。词的概率可以通过该词在训练数据集中的相对词频来计算。例如， :math:`P(w_1)` 可以计算为 :math:`w_1` 在训练数据集中的词频（词出现的次数）与训练数据集的总词数之比。因此，根据条件概率定义，一个词在给定前几个词的情况下的条件概率也可以通过训练数据集中的相对词频计算。例如， :math:`P(w_2 \mid w_1)` 可以计算为 :math:`w_1, w_2` 两词相邻的频率与 :math:`w_1` 词频的比值，因为该比值即 :math:`P(w_1, w_2)` 与 :math:`P(w_1)` 之比；而 :math:`P(w_3 \mid w_1, w_2)` 同理可以计算为 :math:`w_1` 、 :math:`w_2` 和 :math:`w_3` 三词相邻的频率与 :math:`w_1` 和 :math:`w_2` 两词相邻的频率的比值。以此类推。

n元语法
######################

-  :math:`N` 元语法是基于 :math:`n-1` 阶马尔可夫链的概率语言模型，其中 :math:`n` 权衡了计算复杂度和模型准确性。
- 当序列长度增加时，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。 :math:`n` 元语法通过马尔可夫假设（虽然并不一定成立）简化了语言模型的计算。这里的 **马尔可夫假设** 是指一个词的出现只与前面 :math:`n` 个词相关，即 :math:`n` 阶马尔可夫链（Markov chain of order  :math:`n` ）。如果 :math:`n=1` ，那么有 :math:`P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)` 。如果基于 :math:`n-1` 阶马尔可夫链，我们可以将语言模型改写为

.. math::

	P(w_1, w_2, \ldots, w_T) \approx \prod_{t=1}^T P(w_t \mid w_{t-(n-1)}, \ldots, w_{t-1})


- 以上也叫 :math:`n` 元语法（ :math:`n` -grams）。它是基于 :math:`n - 1` 阶马尔可夫链的概率语言模型。当 :math:`n` 分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列 :math:`w_1, w_2, w_3, w_4` 在一元语法、二元语法和三元语法中的概率分别为

.. math::

	\begin{aligned}
	P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\
	P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\
	P(w_1, w_2, w_3, w_4) &=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .
	\end{aligned}


- 当 :math:`n` 较小时， :math:`n` 元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当 :math:`n` 较大时， :math:`n` 元语法需要计算并存储大量的词频和多词相邻频率。

时序数据的采样
######################

- 时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。该样本的标签序列为这些字符分别在训练集中的下一个字符，即“要”“有”“直”“升”“机”。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。
- **随机采样** 在随机采样中，每个样本是原始序列上任意截取的一段序列。相邻的两个随机小批量在原始序列上的位置不一定相毗邻。因此，我们无法用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态。在训练模型时，每次随机采样前都需要重新初始化隐藏状态。
- **相邻采样** 用一个小批量最终时间步的隐藏状态来初始化下一个小批量的隐藏状态，从而使下一个小批量的输出也取决于当前小批量的输入，并如此循环下去。这对实现循环神经网络造成了两方面影响：一方面， 在训练模型时，我们只需在每一个迭代周期开始时初始化隐藏状态；另一方面，当多个相邻小批量通过传递隐藏状态串联起来时，模型参数的梯度计算将依赖所有串联起来的小批量序列。同一迭代周期中，随着迭代次数的增加，梯度的计算开销会越来越大。

困惑度
######################

- 通常使用困惑度（perplexity）来评价语言模型的好坏。
- 困惑度是对交叉熵损失函数做指数运算后得到的值。

	- 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；
	- 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；
	- 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。
	- 显然，任何一个有效模型的困惑度必须小于类别个数。
